{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid token (<ipython-input-1-78b5be64c37a>, line 320)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-78b5be64c37a>\"\u001b[1;36m, line \u001b[1;32m320\u001b[0m\n\u001b[1;33m    l_flat = Flatten()(1_pool) # flatten to 1 dimension; if using l_drop you may lose too much data as the dataset is small\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid token\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# ## CNN for Text Classification\n",
    "# Implementation of *Convolutional Neural Networks for Sentence Classification* (Yoon Kim, 2014).\n",
    "# \n",
    "# In his [paper](https://arxiv.org/abs/1408.5882), Yoon Kim proposed several techniques to achieve good text classification accuracy with minimal hyper-parameter tuning.\n",
    "# \n",
    "# This notebook consist of 4 main sections:\n",
    "# \n",
    "# 1. Preparing the data\n",
    "# 2. Implementing Yoon Kim's CNN model\n",
    "# 3. Training the model\n",
    "# 4. Evaluating the model\n",
    "\n",
    "# **Key Model Parameters**\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100000 # max no. of words for tokenizer # tokenizer: add a number value to each unique word\n",
    "MAX_SEQUENCE_LENGTH = 30 # max length of each entry (sentence), including padding\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 100 # embedding dimensions for word vectors (word2vec/GloVe)\n",
    "# download GloVe file from\n",
    "# https://tlkh.design/downloads/glove.6B.100d.txt.zip\n",
    "# and place it in glove/\n",
    "GLOVE_DIR = \"glove/glove.6B.\"+str(EMBEDDING_DIM)+\"d.txt\"\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, sys, os, csv, keras, pickle\n",
    "from keras import regularizers, initializers, optimizers, callbacks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "'''\n",
    "\n",
    "# ### 1. Prepare the data\n",
    "# **Read from dataset**\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# from nltk.corpus import stopwords # stopwords: things that don't really have any meaning for \n",
    "                                  # the purposes of our classification; only there to make things grammatically correct.\n",
    "                                  # for example, stopwords are words like \"the\", \"and\", etcetera.\n",
    "def clean_text(text):\n",
    "    output = \"\"\n",
    "    text = str(text).replace(\"\\n\", \"\") # get rid of all newlines\n",
    "    text = re.sub(r'[^\\w\\s]','',text).lower().split(\" \") # turn it into a list\n",
    "    for word in text: # for each word in the list of words\n",
    "        if word not in stopwords.words(\"english\"): # if word is not a stopword\n",
    "            output = output + \" \" + word # add word to output string\n",
    "    return str(output.strip())[1:-3].replace(\"  \", \" \") # remove leading and trailing whitespace\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "texts, labels = [], [] # empty lists for the sentences and labels\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__)) # get directory of current python file\n",
    "data_path_neg = os.path.join(dir_path, \"datasets\", \"stanford_movie_neg.txt\") # negative datasets' file path\n",
    "data_path_pos = os.path.join(dir_path, \"datasets\", \"stanford_movie_pos.txt\") # positive datasets' file path\n",
    "\n",
    "print(data_path_neg, data_path_pos)\n",
    "\n",
    "raise\n",
    "\n",
    "data_neg = open(data_path_neg, \"rb\") \n",
    "for line in data_neg: \n",
    "    texts.append(clean_text(line)) # removing all the stopwords\n",
    "    labels.append(int(0)) # label all negative texts with 0\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "data_pos = open(data_path_pos, \"rb\") \n",
    "for line in data_pos: \n",
    "    texts.append(clean_text(line))\n",
    "    labels.append(int(1)) # label all positive texts with 1\n",
    "\n",
    "\n",
    "## now we are left with texts for the cleaned corpus, and labels for the labelling of pos and neg. neg = 0, pos = 1\n",
    "\n",
    "# In[6]:\n",
    "## not very useful for the training. \n",
    "\n",
    "# print(\"Sample positive:\", texts[0], labels[0])\n",
    "# print(\"Sample negative:\", texts[9000], labels[9000])\n",
    "\n",
    "\n",
    "# **Word Tokenizer**\n",
    "\n",
    "# In[7]:\n",
    "\"\"\"\n",
    "class Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, **kwargs)\n",
    "Text tokenization utility class.\n",
    "\n",
    "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
    "\n",
    "# Arguments\n",
    "\n",
    "    num_words: the maximum number of words to keep, based  \n",
    "        on word frequency. Only the most common `num_words` words will  \n",
    "        be kept.  \n",
    "    filters: a string where each element is a character that will be  \n",
    "        filtered from the texts. The default is all punctuation, plus  \n",
    "        tabs and line breaks, minus the `'` character.  \n",
    "    lower: boolean. Whether to convert the texts to lowercase.  \n",
    "    split: str. Separator for word splitting.  \n",
    "    char_level: if True, every character will be treated as a token.  \n",
    "    oov_token: if given, it will be added to word_index and used to  \n",
    "        replace out-of-vocabulary words during text_to_sequence calls  \n",
    "By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized.\n",
    "\n",
    "0 is a reserved index that won't be assigned to any word.\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) # num_words is the max number of words that the tokenizer will recognize\n",
    "tokenizer.fit_on_texts(texts) # tokenizer will create a new entry for every word it encounters\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # why pickle? certain things cannot be saved as str or int types. \n",
    "    # pickle lets us serialize these objects to be saved and reused later.\n",
    "print(\"[i] Saved word tokenizer to file: tokenizer.pickle\") \n",
    "\n",
    "with open('tokenizer.pickle', 'rb') as handle:    \n",
    "    tokenizer = pickle.load(handle) # load a previously generated Tokenizer\n",
    "\n",
    "\n",
    "# **Generate the array of sequences from dataset**\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts) # convert the text corpus to sequences. the main point of tokenizer\n",
    "word_index = tokenizer.word_index # word_index: unique tokens\n",
    "print('[i] Found %s unique tokens.' % len(word_index))\n",
    "data_int = pad_sequences(sequences, padding='pre', maxlen=(MAX_SEQUENCE_LENGTH-5)) # convert to 2D np array\n",
    "data = pad_sequences(data_int, padding='post', maxlen=(MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "\"\"\"\n",
    "pad_sequences Documentation:\n",
    "https://keras.io/preprocessing/sequence/\n",
    "\n",
    "Pads sequences to the same length.\n",
    "\n",
    "This function transforms a list of num_samples sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the length of the longest sequence otherwise.\n",
    "\n",
    "Sequences that are shorter than num_timesteps are padded with value at the end.\n",
    "\n",
    "Sequences longer than num_timesteps are truncated so that they fit the desired length. The position where padding or truncation happens is determined by the arguments padding and  truncating, respectively.\n",
    "\n",
    "Pre-padding is the default.\n",
    "\n",
    "Arguments\n",
    "\n",
    "sequences: List of lists, where each element is a sequence.\n",
    "maxlen: Int, maximum length of all sequences.\n",
    "dtype: Type of the output sequences.\n",
    "padding: String, 'pre' or 'post': pad either before or after each sequence.\n",
    "truncating: String, 'pre' or 'post': remove values from sequences larger than maxlen, either at the beginning or at the end of the sequences.\n",
    "value: Float, padding value.\n",
    "\n",
    "Returns\n",
    "\n",
    "x: Numpy array with shape (len(sequences), maxlen)\n",
    "\n",
    "Raises\n",
    "\n",
    "ValueError: In case of invalid values for truncating or padding, or in case of invalid shape for a sequences entry.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# **Create the train-validation split**\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "labels = to_categorical(np.asarray(labels)) # Converts a class vector (integers) to binary class matrix.\n",
    "print('[i] Shape of data tensor:', data.shape)\n",
    "print('[i] Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0]) # np.arange(5) returns array([0, 1, 2, 3, 4])\n",
    "np.random.shuffle(indices) # shuffle contents of np array\n",
    "data = data[indices] \n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('[i] Number of entries in each category:')\n",
    "print(\"[+] Training:\",y_train.sum(axis=0))\n",
    "print(\"[+] Validation:\",y_val.sum(axis=0))\n",
    "\n",
    "# separate the data that you are going to use for training from the one you will use for validating\n",
    "\n",
    "# **What does the data look like?**\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "print(\"Tokenized sequence:\\n\", data[0])\n",
    "print(\"\")\n",
    "print(\"One-hot label:\\n\", labels[0])\n",
    "\n",
    "\n",
    "# ### 2. Create the model\n",
    "# Yoon Kim's model has several notable features:\n",
    "# ![model-structure](notebook_imgs/yoon_kim_structure.png)\n",
    "# * two sets of word embeddings for what he terms a **\"multi-channel\" approach**.\n",
    "#   * One of the word embeddings will be frozen (**\"static channel\"**), \n",
    "#     and one will be modified during the training process (**\"non-static channel\"**). \n",
    "# * multiple convolutional kernel sizes\n",
    "# \n",
    "# We will now start to create the model in `Keras`.\n",
    "\n",
    "# **Load word embeddings into an `embeddings_index`**\n",
    "# \n",
    "# Create an index of words mapped to known embeddings, by parsing the data dump of pre-trained embeddings.\n",
    "# \n",
    "# We use a set from [pre-trained GloVe vectors from Stanford](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR, encoding=\"utf8\")\n",
    "print(\"[i] (long) Loading GloVe from:\",GLOVE_DIR,\"...\",end=\"\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index[word] = np.asarray(values[1:], dtype='float32') # create an embedding index\n",
    "f.close()\n",
    "print(\"Done.\\n[+] Proceeding with Embedding Matrix...\", end=\"\")\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\" Completed!\")\n",
    "# the 0th dimension for keras is equal to the number of entries of the batch\n",
    "# 1st dimension is the word sequence\n",
    "# 2nd dimension is the word vectors\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# second embedding matrix for non-static channel\n",
    "embedding_matrix_ns = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix_ns[i] = embedding_vector\n",
    "\n",
    "\n",
    "# **Create the `Embedding` layers**\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') # input to the model\n",
    "\n",
    "# static channel\n",
    "embedding_layer_frozen = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "embedded_sequences_frozen = embedding_layer_frozen(sequence_input)\n",
    "\n",
    "# non-static channel\n",
    "embedding_layer_train = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix_ns],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences_train = embedding_layer_train(sequence_input)\n",
    "\n",
    "l_lstm_1 = Concatenate(axis=1)([embedded_sequences_frozen, embedded_sequences_train])\n",
    "\n",
    "\n",
    "# **Create the CNN layer with multiple kernel (filter) sizes**\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "l_conv_3 = Conv1D(filters=128,kernel_size=3,activation='relu',kernel_regularizer=regularizers.l2(0.001))(l_lstm_1)\n",
    "l_conv_4 = Conv1D(filters=128,kernel_size=4,activation='relu',kernel_regularizer=regularizers.l2(0.001))(l_lstm_1)\n",
    "l_conv_5 = Conv1D(filters=128,kernel_size=5,activation='relu',kernel_regularizer=regularizers.l2(0.001))(l_lstm_1)\n",
    "# filters: number of matrices\n",
    "# kernel_size: size of the matrix; for kernel_size=3, matrix is a 3 x 1 matrix. (Because it is 1D in this case.)\n",
    "# activation: activation function. we are use relu in this case.\n",
    "l_conv = Concatenate(axis=1)([l_conv_3, l_conv_4, l_conv_5]) # concat all the 3 outputs\n",
    "\n",
    "\n",
    "# Followed by the rest of the model (boring!!)\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "l_pool = MaxPooling1D(4)(l_conv) # selects the strongest output out of every 4 outputs\n",
    "l_drop = Dropout(0.2)(l_pool) # dropout 20% of the output of l_pool\n",
    "# l_flat = Flatten()(1_pool) # flatten to 1 dimension; if using l_drop you may lose too much data as the dataset is small\n",
    "l_dense = Dense(32, activation='relu')(l_flat) # 32 perceptrons\n",
    "preds = Dense(2, activation='softmax')(l_dense) #follows the number of classes\n",
    "\n",
    "\n",
    "# **Compile the model into a static graph for training**\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# **Visualisation**\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "'''\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "'''\n",
    "# not necessary. uncomment if needed.\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# Keras callback functions\n",
    "tensorboard = callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=16, write_grads=True , write_graph=True)\n",
    "model_checkpoints = callbacks.ModelCheckpoint(\"checkpoint-{val_loss:.3f}.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=0)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# remove TensorFlow logs directory and old checkpoints\n",
    "get_ipython().system('rm -r logs *.h5')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#model = keras.models.load_model(\"checkpoint-0.91.h5\") # in case you ever want to load from a checkpoint\n",
    "\n",
    "\n",
    "# ### 3. Train the model\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "print(\"Training Progress:\")\n",
    "model_log = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "            epochs=12, batch_size=32, # around 32 for CPU, higher for GPU\n",
    "            callbacks=[tensorboard, model_checkpoints]) # publish to tensorboard and save automatic checkpoints\n",
    "# 1 epoch = 1 run of the program\n",
    "#pd.DataFrame(model_log.history).to_csv(\"history.csv\") # save the training progress to a csv file\n",
    "\n",
    "\n",
    "# ### 4. Evaluate the model\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "get_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'retina'\")\n",
    "\n",
    "plt.plot(model_log.history['acc'])\n",
    "plt.plot(model_log.history['val_acc'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model_log.history['loss'])\n",
    "plt.plot(model_log.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools, pickle\n",
    "\n",
    "classes = [\"positive\", \"negative\"]\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "Y_test = np.argmax(y_val, axis=1) # Convert one-hot to index\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "cnf_matrix = confusion_matrix(Y_test, y_pred_class)\n",
    "print(classification_report(Y_test, y_pred_class, target_names=classes))\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, labels,\n",
    "                          normalize=True,\n",
    "                          title='Confusion Matrix (Validation Set)',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_confusion_matrix(cnf_matrix, labels=classes)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
